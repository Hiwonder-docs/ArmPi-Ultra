# 14. ArmPi Ultra with Sliding Rail

## 14.1 Sliding Rail Installation

Below is a schematic overview of the installation steps:

**Step 1: Install the Tank Track**
Secure the tank track onto the sliding rail using two M4×6 pan head machine screws.

<img class="common_img" src="../_static/media/chapter_14/media/image3.png" style="width:500px" />

**Step 2: Install the Motor Fixing Cover**

Attach the motor fixing cover to the sliding rail using two M3×6 pan head machine screws.

<img class="common_img" src="../_static/media/chapter_14/media/image4.png" style="width:500px" />

<img class="common_img" src="../_static/media/chapter_14/media/image5.png" style="width:500px"  />

**Step 3: Install the Sliding Rail Base Plate**

Secure the base plate to the sliding rail using four M5×12 countersunk machine screws and four 5.2×7×4 standoffs.

<img class="common_img" src="../_static/media/chapter_14/media/image6.png" style="width:500px" />

<img class="common_img" src="../_static/media/chapter_14/media/image7.png" style="width:500px"  />

**Step 4: Install Brass Standoff**

Fasten two M3×15 double-pass brass standoffs to the ArmPi Ultra base using two M3×6 pan head machine screws.

<img class="common_img" src="../_static/media/chapter_14/media/image8.png" style="width:500px" />

**Step 5: Install the Robotic Arm**

Fasten the robotic arm to the sliding rail base using two M3×12 pan head machine screws and secure them with the corresponding nuts.

<img class="common_img" src="../_static/media/chapter_14/media/image9.png" style="width:500px" />

**Step 6: Attach the Tank Track to the Robotic Arm**

Use two M3*6 pan head machine screws to secure the tank track assembly to the ArmPi Ultra robotic arm.

<img class="common_img" src="../_static/media/chapter_14/media/image10.png" style="width:500px" />



<img class="common_img" src="../_static/media/chapter_14/media/image11.png" style="width:500px" />

**Step 7: Final Assembly and Wiring Instructions**

Once fully assembled, the setup should appear as shown in the diagram below.

<img class="common_img" src="../_static/media/chapter_14/media/image12.png" style="width:500px" />

Connect the 3-pin cable from the sliding rail to the 3-pin terminal (port 1) on the STM32 expansion board for power supply. Connect the 4-pin cable from the sliding rail to the I2C port (port 2) on the expansion board. As shown in the figure below:

<img class="common_img" src="../_static/media/chapter_14/media/image13.png" style="width:500px" />


## 14.2 Selecting the Robot Type

ArmPi Ultra’s expansion accessories come in three types: Mecanum chassis, sliding rail, and conveyor belt. After installation, you must switch the device version according to the installed accessory for proper operation.

> [!NOTE]
>
> **Note:** If you don’t switch or select the wrong version, the motor may run unpredictably, causing malfunctions or even damaging the device.

Step-by-step instructions:

1)  Start the robotic arm, and access the robot system desktop using VNC. To get detailed instructions on remote control software connection, please refer to the tutorials saved in **[1.ArmPi Ultra User Manual-> 1.6 Development Environment Setup and Configuration]()** in the ArmPi Ultra User Manual.

2)  Double-click the model configuration tool on the desktop. <img class="common_img" src="../_static/media/chapter_14/media/image14.png" style="width:50px" />

3)  Select the appropriate options based on the robotic arm version, camera version, and accessory type:

<img class="common_img" src="../_static/media/chapter_14/media/image15.png" style="width:500px" />

**Mecanum** refers to the Mecanum chassis. **Slide_Rails** refers to the sliding rail. **Conveyor_Belt** refers to the conveyor belt.

4.  After making your selection, click **Save** to save the configuration.

<img class="common_img" src="../_static/media/chapter_14/media/image16.png" style="width:500px" />

5.  Click **Apply** to reload the configuration.

<img class="common_img" src="../_static/media/chapter_14/media/image17.png" style="width:500px" />

Wait for the buzzer to beep once—this indicates the restart is complete and the new configuration is now active.

## 14.3 Position Calibration

> [!NOTE]
>
> This section applies specifically to the sliding rail configuration. Please ensure the robotic arm is calibrated for mechanical deviations while placed on a flat table before mounting it on the sliding rail.

Position calibration is the process of converting coordinates from the camera’s visual feedback to the real-world coordinates of the robotic arm. Through this calibration, the system can accurately determine the actual distance between the robotic arm and the wooden block, enabling precise operations like sorting and object placement.

Please first refer to section **[1.ArmPi Ultra User Manual -> 1.8 Position Adjustment for Object Gripping and Placing]()** and Placing in the ArmPi Ultra User Manual to perform position calibration.

## 14.4 Wireless Controller Control

### 14.4.1 Preparation

1.  Before powering on the device, make sure the wireless controller receiver is properly inserted. This can be ignored if the receiver was pre-inserted at the factory.

2.  Pay attention to battery polarity when placing the batteries.

<img class="common_img" src="../_static/media/chapter_14/media/image18.png" style="width:500px"  />

3.  Each time the robot is powered on, the app auto-start service will launch which includes the wireless handle control service. If this service has not been closed, no additional actions are needed—simply connect and control.

4.  Since signals from wireless controller can interfere with each other, it is recommended not to use this function when multiple robots are in the same area, to avoid misconnection or unintended control.

5.  After turning on the wireless controller, if it does not connect to the robot within 30 seconds, or remains unused for 5 minutes after connection, it will enter sleep mode automatically. To wake up the wireless handle and exit sleep mode, press the **START** button.

### 14.4.2 Device Connection

1.  After the robot powers on, slide the wireless controller switch to the **ON** position. At this point, the red and green LED indicators on the wireless controller will start flashing simultaneously.

2.  Wait a few seconds for the robot and wireless controller to pair automatically. Once pairing is successful, the green LED will remain solid while the red LED turns off.

<img class="common_img" src="../_static/media/chapter_14/media/image19.png" style="width:500px" />

### 14.4.3 Control Modes

The wireless handle supports two control modes: Coordinate Mode and Single Servo Mode. After a successful connection, the default mode is Coordinate Mode.

- **Single Servo Mode:** In this mode, the wireless controller buttons can be used to control the forward and reverse rotation of individual servos on the robotic arm.

<img class="common_img" src="../_static/media/chapter_14/media/image20.png" style="width:500px" />

Button Functions in Single Servo Mode:

| **Button** | **Function (from the robotic arm’s first-person perspective)** |
|:--:|:--:|
| START | Reset the robotic arm |
| SELECT+START | Switch control mode (Single Servo / Coordinate) |
| UP / ↑ | Raise Servo 5 |
| DOWN / ↓ | Lower Servo 5 |
| LEFT / ← | Rotate Servo 6 to the left |
| RIGHT / → | Rotate Servo 6 to the right |
| Y | Lower Servo 4 |
| A | Raise Servo 4 |
| B | Rotate Servo 2 to the right (Gripper turns right) |
| X | Rotate Servo 2 to the left (Gripper turns left) |
| L1 | Open the gripper (Servo 1) |
| L2 | Close the gripper (Servo 1) |
| R1 | Raise Servo 3 |
| R2 | Lower Servo 3 |
| Left Joystick LEFT | Move the sliding rail to the left |
| Left Joystick RIGHT | Move the sliding rail to the right |

- **Coordinate Mode:** In Coordinate Mode, the robotic arm moves as a whole along the X, Y, and Z axes and can also adjust its tilt angle based on button inputs.

<img class="common_img" src="../_static/media/chapter_14/media/image21.png" style="width:500px" />

Switching Between Modes: To switch between modes, press both **SELECT** and **START** buttons. A sound prompt indicates the switch was successful.

1.  Two beeps: Switched from Single Servo Mode to Coordinate Mode.

2.  One beep: Switched from Coordinate Mode to Single Servo Mode.

Button Functions in Coordinate Mode:

| **Button** | **Function (from the robotic arm’s first-person perspective)** |
|:--:|:--:|
| START | Reset the robotic arm |
| SELECT+START | Switch control mode (Single Servo / Coordinate) |
| UP / ↑ | Move arm in the positive X direction (forward) |
| DOWN / ↓ | Move arm in the negative X direction (backward) |
| LEFT / ← | Move arm in the positive Y direction (left) |
| RIGHT / → | Move arm in the negative Y direction (right) |
| Y | Close the gripper (Servo 1) |
| A | Open the gripper (Servo 1) |
| B | Rotate Servo 2 to the right (Gripper turns right) |
| X | Rotate Servo 2 to the left (Gripper turns left) |
| L1 | Move arm upward along Z axis |
| L2 | Move arm downward along Z axis |
| R1 | Increase gripper pitch angle |
| R2 | Decrease gripper pitch angle |
| Left Joystick LEFT | Move the sliding rail to the left |
| Left Joystick RIGHT | Move the sliding rail to the right |

## 14.5 Color Sorting

### 14.5.1 Overview

In this experiment, the camera uses OpenCV algorithms to detect color blocks of a target color and calculates the center position of each block. Next, an inverse kinematics algorithm is applied to compute the pulse widths for the robotic arm’s servos, enabling the arm to grasp the target color block. Finally, the sliding rail moves to the designated drop-off location based on the color of the block, and the arm performs a placing action to complete the sorting process.

### 14.5.2 Operations

> [!NOTE]
>
> **Note:** When entering commands, be sure to use correct case and spacing. You can use the Tab key to auto-complete keywords.

1. Refer to the tutorial in section **6. Development Environment Setup and Configuration** of the ArmPi Ultra User Manual to establish a connection between the robotic arm and the remote desktop tool.

2. Click on <img class="common_img" src="../_static/media/chapter_14/media/image22.png" style="width:50px" /> to launch the command bar, enter the command, and press **Enter** to disable the auto-start service.

   ```py
   ~/.stop_ros.sh
   ```

3. Enter the following command and press **Enter** to enable color sorting feature.

   ```py
   ros2 launch example color_sorting_stepper.launch.py
   ```

4. To stop the program, press **Ctrl + C** in the terminal window. If the program does not stop immediately, repeat this step until it terminates.

5. After completing the feature, you need to start the app service<img class="common_img" src="../_static/media/chapter_14/media/image22.png" style="width:50px" />. **Otherwise, future app features may not function properly.** In the terminal, enter the command and press **Enter** to start the app service.

   ```py
   ros2 launch bringup bringup.launch.py
   ```

6)  Once the app service is successfully started, the robotic arm will return to its initial pose, and the buzzer will beep once.

### 14.5.3 Project Outcome

After starting the program, place the target color blocks within the robotic arm’s visual recognition area. The robotic arm will detect and pick up each color block one by one, then place them in the corresponding locations.

### 14.5.4 Program Brief Analysis

* **launch File Analysis**

The launch files are located at: **~/ros2_ws/src/example/example/stepper/color_sorting_stepper.launch.py**

**1. launch_setup Function**

<img class="common_img" src="../_static/media/chapter_14/media/image26.png" style="width:700px" />

Loads the **launch/depth_camera.launch.py** file from the **peripherals** package to start the depth camera node, which provides RGB image and depth data, which is the visual input for color recognition. Loads the **launch/armpi_ultra.launch.py** file from the **sdk** package to start the low-level control services of the robotic arm, including servo driving and kinematics calculation, providing hardware control support for the sorting operation. Starts the **color_sorting_stepper** executable from the **example** package to control the sliding rail based on the visual recognition results and complete the automated color sorting process, with logs output to the screen.

**2. generate_launch_description Function**

<img class="common_img" src="../_static/media/chapter_14/media/image27.png" style="width:500px" />

Creates and returns a LaunchDescription object, calling launch_setup via OpaqueFunction as the standard entry point for the ROS 2 launch file.

**3. Main Function**

<img class="common_img" src="../_static/media/chapter_14/media/image28.png" style="width:500px" />

Creates a LaunchDescription object and a LaunchService service. The launch description is added to the service and executed, enabling the startup of the entire system manually.

* **Python File Analysis**

Python files locate at: **~/ros2_ws/src/example/example/stepper/include/color_sorting_stepper.py**

**1. Import the Necessary Libraries**

<img class="common_img" src="../_static/media/chapter_14/media/image29.png" style="width:700px" />

1.  **`sdk.common`**: Modules from the robotic arm SDK (Software Development Kit).

2.  **`servo_controller_msgs.msg.ServosPosition`**: Message type used to control servo motor positions, typically for servo control.

3.  **`kinematics.kinematics_control.set_pose_target`**: Function to set the robot’s target pose of position and orientation, based on kinematics calculations.

4.  **`kinematics_msgs.srv.GetRobotPose and SetRobotPose`**: Service interfaces used to get and set the robot’s pose information.

5.  **`servo_controller.bus_servo_control.set_servo_position`**: Function to set servo motor positions, primarily used for controlling robotic grasping or movement.

**2. Target Position Specification**

<img class="common_img" src="../_static/media/chapter_14/media/image30.png" style="width:500px" />

This is a dictionary used to store target positions for different colors, such as red, green, and blue. This structure simplifies quickly locating the corresponding position based on color during the robot’s tasks. Sliding rail position range is from 0 (leftmost end) to 5200 (rightmost end).

**3. ColorSortingNode Class Initialization**

<img class="common_img" src="../_static/media/chapter_14/media/image31.png" style="width:700px" />

It initializes nodes, sets up image processing subscriptions and publications, configures stepper motor control, and prepares service clients for robot kinematics. It loads color range parameters from a LAB color space configuration file, creates an image queue for buffer management, and uses multithreading to allow the main logic to run in parallel, enabling coordinated image processing and motion control.

**4. send_request Function**

<img class="common_img" src="../_static/media/chapter_14/media/image32.png" style="width:500px" />

It encapsulates the service call logic, sends the request, and waits synchronously for the result to ensure reliable communication with services such as robotic arm kinematics and stepper motor control.

**5. camera_info_callback Function**

<img class="common_img" src="../_static/media/chapter_14/media/image33.png" style="width:700px" />

It receives the camera intrinsic parameters, extract the camera matrix K and distortion coefficients D, and optimize the intrinsics using cv2.getOptimalNewCameraMatrix to prepare for coordinate conversion.

**6. adaptive_threshold Function**

<img class="common_img" src="../_static/media/chapter_14/media/image34.png" style="width:700px" />

It applies adaptive Gaussian thresholding to the grayscale image to generate a binary image and enhance the edge features of the object.

**7. canny_proc Function**

<img class="common_img" src="../_static/media/chapter_14/media/image35.png" style="width:700px" />

It uses Canny edge detection to extract image edges and applies dilation to refine the edges, producing an edge mask.

**8. get_top_surface Function**

<img class="common_img" src="../_static/media/chapter_14/media/image36.png" style="width:500px" />

It combines image scaling, grayscale conversion, blurring, thresholding, and edge detection to extract the top surface of the object and reduce background interference.

**9. image_callback Function**

<img class="common_img" src="../_static/media/chapter_14/media/image37.png" style="width:500px" />

It converts the incoming ROS image message of type sensor_msgs/Image to an OpenCV RGB image and stores it in the image queue, discarding the oldest frame when the queue is full for subsequent processing.

**10. init_process Function**

<img class="common_img" src="../_static/media/chapter_14/media/image38.png" style="width:700px" />

It cancels any existing timers, resets the stepper motor, and adjusts the robotic arm servos to the initial pose, waiting 1.5 seconds to ensure completion. It starts the main loop thread and the transport control thread transport_thread. It loads the configuration file **transform.yaml**, parses the calibration parameters including corner coordinates, extrinsics, and white area poses, and computes the image ROIs to limit the recognition range.

**11. image_processing Function**

<img class="common_img" src="../_static/media/chapter_14/media/image39.png" style="width:700px" />

It retrieves images from the queue and processes them within the ROI regions using masks to limit the recognition range. It converts the images to the LAB color space and uses color thresholds loaded from **lab_config.yaml** to detect red, green, and blue objects. It extracts contours for each color object, calculates area, center coordinates, angles, and other features, and filters out targets that are too small or too large. It marks the detected objects on the image including center and contours, and returns a list of targets with color, coordinates, and angles, along with the annotated image.

**12. get_object_world_position Function**

<img class="common_img" src="../_static/media/chapter_14/media/image40.png" style="width:700px" />

It calculates the world coordinates of the object using the camera intrinsics K and extrinsics extristric through a projection matrix. It applies scaling factors and offsets from the calibration file **calibration.yaml** to optimize coordinate accuracy and returns the object's position in the world coordinate system.

**13. calculate_pick_grasp_yaw Function**

<img class="common_img" src="../_static/media/chapter_14/media/image41.png" style="width:700px" />

It computes the grasping angle yaw based on the object's world coordinates and optimizes the grasping direction according to the gripper size and target pose.

**14. calculate_place_grasp_yaw Function**

<img class="common_img" src="../_static/media/chapter_14/media/image42.png" style="width:500px" />

It calculates the placement angle to ensure the object is correctly placed at the target position.

**15. transport_thread Function**

<img class="common_img" src="../_static/media/chapter_14/media/image43.png" style="width:700px" />

When start_transport is True, it executes the pick-transport-place process, and calls pick_and_place.pick to control the robotic arm to grasp the object. It moves the object to the target position using the stepper motor service move_stepper according to the color from TARGET_POSITION. It calls pick_and_place.place to control the robotic arm to place the object. It resets the stepper motor, returns the robotic arm to the initial pose, and resets the state variables.

**16. main Method**

<img class="common_img" src="../_static/media/chapter_14/media/image44.png" style="width:700px" />

It continuously calls image_processing to obtain the list of targets and monitor their status. It triggers the transport process by setting start_transport = True when the target position is stable for 15 consecutive frames with displacement less than 0.003 meters. It resets the target status if the target is lost for more than 5 consecutive frames. It displays the processed images in real time and supports keyboard interactions using cv2.waitKey.

**17. Main Function**

<img class="common_img" src="../_static/media/chapter_14/media/image45.png" style="width:500px" />

It serves as the entry point to start and run the ROS2 node. It creates an instance of ColorSortingNode, uses a MultiThreadedExecutor to support multithreaded processing, starts the node, and ensures proper resource cleanup upon termination.

## 14.6 Waste Classification

### 14.6.1 Overview

In this section, a trained YOLOv8 model is used to classify waste blocks through a camera. Then, inverse kinematics is applied to control the robotic arm to grasp the waste block and sort it into the corresponding trash bin.

**Step 1: Object Detection and Classification**

First, subscribe to the real-time image data published by the camera node and convert the received data into NumPy format. After conversion, the image is processed by the YOLOv8 network through resizing, transposing, and array expansion to obtain detection results.

The detected image coordinates of the waste cards are then transformed back to the original image coordinates. Based on the detected object name, the waste is categorized according to predefined rules, and the corresponding waste category name is retrieved. Finally, a bounding box is drawn around each piece of waste, and its name and detection confidence are displayed on the screen.

**Step 2: Grasping and Sorting**

To ensure the reliability of the results, multiple detections are performed to confirm the target object. The waste card with the highest confidence in the image is selected as the primary sorting target. Repeated detections are carried out to verify the object's consistency. Once the target is confirmed, the robotic arm begins the sorting process by converting the target card's pixel coordinates into world coordinates.

The robotic arm is then controlled to move to the location of the waste block and perform the grasping operation. Based on the identified waste category, the sliding rail moves the arm to the designated drop-off position. Finally, the robotic arm places the waste card into the corresponding bin, completing the waste classification task.

### 14.6.2 Operations

> [!NOTE]
>
> **Note:** When entering commands, be sure to use correct case and spacing. You can use the Tab key to auto-complete keywords.

1. Refer to the tutorial in section **[1.ArmPi Ultra User Manual-> 1.6 Development Environment Setup and Configuration]()** of the ArmPi Ultra User Manual to establish a connection between the robotic arm and the remote desktop tool.

2. Click the icon <img class="common_img" src="../_static/media/chapter_14/media/image22.png" style="width:50px" /> to launch the command bar, enter the command, and press **Enter** to disable the auto-start service.

   ```py
   ~/.stop_ros.sh
   ```

3. Enter the following command and press **Enter** to enable waste classification feature.

   ```py
   ros2 launch example waste_classification_stepper.launch.py
   ```

4. To stop the program, press **Ctrl + C** in the terminal window. If the program does not stop immediately, repeat this step until it terminates.

5. After completing the feature, you need to start the app service<img class="common_img" src="../_static/media/chapter_14/media/image22.png" style="width:50px" />. **Otherwise, future app features may not function properly.** In the terminal, enter the command and press **Enter** to start the app service.

   ```py
   ros2 launch bringup bringup.launch.py
   ```

6)  Once the app service is successfully started, the robotic arm will return to its initial pose, and the buzzer will beep once.

### 14.6.3 Project Outcome

After the program starts, once the camera detects a waste card, the corresponding category name will be displayed on the screen. Each category is highlighted with a rectangle of a specific color. Hazardous waste is marked in red, Recyclables in blue, Kitchen waste in green, Other waste in gray.

| **Waste Category** | **Card** |
|:--:|----|
| Hazardous Waste (hazardous_waste) | Storage Battery, Marker, Oral Liquid Bottle |
| Recyclable Waste (recyclable_waste) | Plastic Bottle, Umbrella, Toothbrush |
| Food Waste (food_waste) | Banana Peel, Ketchup, Broken Bones |
| Residual Waste (residual_waste) | Cigarette End, Plate, Disposable Chopsticks |

### 14.6.4 Program Brief Analysis

* **launch File Analysis**

The launch files are located at: **~/ros2_ws/src/example/example/stepper/waste_classification_stepper.launch.py**

**1. launch_setup Function**

<img class="common_img" src="../_static/media/chapter_14/media/image47.png" style="width:700px" />

Loads the **launch/depth_camera.launch.py** file from the **peripherals** package to start the depth camera node, which provides RGB image and depth data, which is the visual input for target detection. Loads the **launch/armpi_ultra.launch.py** file from the **sdk** package to start the low-level control services of the robotic arm, including servo driving and kinematics calculation, providing hardware support for the waste classification. It launches the executable **yolov8_node** from the **example** package, which uses the pre-trained model **best_garbage** to detect 12 types of waste, such as banana peels, plastic bottles, and batteries. The parameters include detection classes, device set to CPU, and a confidence threshold of 0.8, with logs output to the screen. It launches the executable **waste_classification_stepper** from the **example** package. It uses the detection results from YOLOv8 to control the stepper motor, driving the mechanical structure to transport different types of waste to their corresponding sorting areas and complete the automated sorting process.

**2. generate_launch_description Function**

<img class="common_img" src="../_static/media/chapter_14/media/image27.png" style="width:500px" />

Creates and returns a LaunchDescription object, calling launch_setup via OpaqueFunction as the standard entry point for the ROS 2 launch file.

**3. Main Function**

<img class="common_img" src="../_static/media/chapter_14/media/image28.png" style="width:500px" />

Creates a LaunchDescription object and a LaunchService service. The launch description is added to the service and executed, enabling the startup of the entire system manually.

* **Python File Analysis**

Python files locate at: **~/ros2_ws/src/example/example/stepper/include/waste_classification_stepper.py**

**1. Import the Necessary Libraries**

<img class="common_img" src="../_static/media/chapter_14/media/image48.png" style="width:700px" />

1.  **from sdk import common**: Imports the **common** module from the **SDK**, which usually contains general-purpose functions and utility tools.

2.  **from servo_controller_msgs.msg import ServosPosition**: Imports the **ServosPosition** message type from the **servo_controller_msgs** message package, used for transmitting servo motor position data.

3.  **from kinematics.kinematics_control import set_pose_target**: Imports the set_pose_target function from the kinematics control module, used to set the target pose of the robot's end effector.

4.  **from kinematics_msgs.srv import GetRobotPose, SetRobotPose**: Imports the **GetRobotPose** and **SetRobotPose** service types from the kinematics message service package, used for getting and setting the robot's pose.

5.  **from servo_controller.bus_servo_control import set_servo_position**: Imports the set_servo_position function from the servo control bus module, used to set the position of servo motors.

**2. Target Position Specification**

<img class="common_img" src="../_static/media/chapter_14/media/image49.png" style="width:500px" />

This is a dictionary used to store target positions for different wastes. This structure simplifies quickly locating the corresponding position based on color during the robot’s tasks. Sliding rail position range is from 0 (leftmost end) to 5200 (rightmost end).

**3. Specified Waste Types:**

<img class="common_img" src="../_static/media/chapter_14/media/image50.png" style="width:500px" />

This is a dictionary used to store different types of waste. These types are used later in the program to determine the placement location.

**4. WasteClassificationNode Class Initialization Function**

<img class="common_img" src="../_static/media/chapter_14/media/image51.png" style="width:700px" />

It initializes the node parameters, including running status, ROI regions, and coordinate transformation parameters. It defines the waste class mapping WASTE_CLASSES, which maps 12 specific types of waste into 4 major categories, and sets the target placement positions TARGET_POSITION, specifying the stepper motor steps for each type of waste.

**5. init_process Function**

<img class="common_img" src="../_static/media/chapter_14/media/image52.png" style="width:700px" />

It cancels any existing timers, resets the stepper motor, and adjusts the robotic arm servos to the initial pose, waiting 1.5 seconds to ensure completion. It starts the YOLOv8 object detection service and launches the main loop thread main as well as the transport control thread transport_thread.

**6. camera_info_callback Function**

<img class="common_img" src="../_static/media/chapter_14/media/image53.png" style="width:500px" />

It receives the camera intrinsic message, extracts the camera matrix intrinsic and distortion coefficients distortion, and provides the basic parameters for converting pixel coordinates to world coordinates.

**7. send_request Function**

<img class="common_img" src="../_static/media/chapter_14/media/image32.png" style="width:500px" />

It encapsulates the service call logic, sends the request, and waits synchronously for the result to ensure reliable communication with services such as robotic arm kinematics and stepper motor control.

**8. image_callback Function**

<img class="common_img" src="../_static/media/chapter_14/media/image54.png" style="width:700px" />

It receives the annotated images output by YOLOv8, draws the ROI regions on the images, and displays the detection results in real time for debugging.

**9. get_roi Function**

<img class="common_img" src="../_static/media/chapter_14/media/image55.png" style="width:700px" />

It loads the configuration file **transform.yaml** and parses the calibration parameters, including corner coordinates, extrinsics, and the poses of the white area. It calculates the ROI regions in the image by projecting world coordinates to the image plane using the camera intrinsic and extrinsic parameters. This limits the detection range, reduces background interference, and improves recognition efficiency.

**10. get_object_world_position Function**

<img class="common_img" src="../_static/media/chapter_14/media/image56.png" style="width:700px" />

It calculates the world coordinates of the waste using the camera intrinsics intrinsic and extrinsics extristric through a projection matrix. It applies scaling factors and offsets from the calibration file **calibration.yaml** to optimize coordinate accuracy and returns the position of the waste in the world coordinate system.

**11. calculate_pick_grasp_yaw Function**

<img class="common_img" src="../_static/media/chapter_14/media/image57.png" style="width:700px" />

It computes the grasping angle yaw based on the object's world coordinates and optimizes the grasping direction according to the gripper size and target pose.

**12. calculate_place_grasp_yaw Function**

<img class="common_img" src="../_static/media/chapter_14/media/image58.png" style="width:500px" />

It calculates the placement angle to ensure the waste is correctly placed at the target sorting position.

**13. transport_thread Function**

<img class="common_img" src="../_static/media/chapter_14/media/image59.png" style="width:700px" />

<img class="common_img" src="../_static/media/chapter_14/media/image60.png" style="width:700px" />

When start_transport is True, it executes the full pick-transport-place process. It stops the YOLOv8 detection to avoid interference during grasping. It calls pick_and_place.pick to control the robotic arm to grasp the waste. It moves the waste to the corresponding sorting area using the stepper motor service move_stepper, retrieving the steps from TARGET_POSITION based on the waste category. It calls pick_and_place.place to control the robotic arm to place the waste. It resets the stepper motor, returns the robotic arm to the initial pose, restarts YOLOv8 detection, and resets the state variables.

**14. main Method**

<img class="common_img" src="../_static/media/chapter_14/media/image61.png" style="width:700px" />

It calls get_roi to initialize the ROI regions and continuously monitors the object detection results. It triggers the transport process by setting start_transport = True when waste is detected and its position is stable for 4 consecutive frames with displacement less than 0.002 meters. The waste category is mapped using WASTE_CLASSES, and the grasping position and angle are calculated. It removes the target if the waste position continues to move for more than 10 frames to avoid invalid grasping.

**15. get_object_callback Function**

<img class="common_img" src="../_static/media/chapter_14/media/image62.png" style="width:700px" />

It subscribes to the YOLOv8 object detection messages ObjectsInfo, extracts the category, coordinates, and angle of the waste, and filters valid targets in target_list to provide data for the main thread.

**16. Main Function**

<img class="common_img" src="../_static/media/chapter_14/media/image63.png" style="width:700px" />

It serves as the entry point to start and run the waste classification ROS2 node. It creates an instance of the WasteClassificationNode, uses a MultiThreadedExecutor to support multithreaded processing, starts the node, and properly cleans up resources upon termination.

## 14.7 Tag Sorting

### 14.7.1 Overview

In this lesson, the system uses a camera to recognize tags and then uses the tag information and its position to control a robotic arm. Based on inverse kinematics calculations, the robotic arm picks up the tagged wooden block and moves it to a designated location via a sliding rail system.

AprilTag is a type of visual localization marker, similar to QR codes or barcodes.

The process begins by subscribing to the camera node to obtain real-time image data. The image is converted from the RGB color space to grayscale, and the camera’s intrinsic parameters are retrieved.

Next, the AprilTag detection function from the corresponding library is called to process the image. This detects AprilTags and retrieves their IDs, the coordinates of the four corner points of each tag, and the coordinates of their center points.

Using the coordinate information from detection, the system calculates the position of each wooden block. Then, inverse kinematics is applied to compute the necessary joint angles for the robotic arm to reach the target position. Finally, the robotic arm performs the pick-up action and the sliding rail transports the wooden block to the position corresponding to the tag ID, where it is placed.

### 14.7.2 Operations

> [!NOTE]
>
> **Note:** When entering commands, be sure to use correct case and spacing. You can use the Tab key to auto-complete keywords.

1. Refer to the tutorial in section **[1.ArmPi Ultra User Manual-> 1.6 Development Environment Setup and Configuration]()** of the ArmPi Ultra User Manual to establish a connection between the robotic arm and the remote desktop tool.

2. Click the icon <img class="common_img" src="../_static/media/chapter_14/media/image22.png" style="width:50px" /> to launch the command bar, enter the command, and press **Enter** to disable the auto-start service.

   ```py
   ~/.stop_ros.sh
   ```

3. Open a new terminal window, enter the following command, and press **Enter** to launch the tag sorting feature.

   ```py
   ros2 launch example tag_sorting_stepper.launch.py
   ```

4. To stop the program, press **Ctrl + C** in the terminal window. If the program does not stop immediately, repeat this step until it terminates.

5. After completing the feature, you need to start the app service<img class="common_img" src="../_static/media/chapter_14/media/image22.png" style="width:50px" />. **Otherwise, future app features may not function properly.** In the terminal, enter the command and press Enter to start the app service.

   ```py
   ros2 launch bringup bringup.launch.py
   ```

6)  Once the app service is successfully started, the robotic arm will return to its initial pose, and the buzzer will beep once.

### 14.7.3 Project Outcome

After the feature is started, place the tagged wooden blocks within the robotic arm’s visual recognition area. The robotic arm will automatically pick up the blocks in the order of tag IDs 1, 2, and 3, and place them at their corresponding target locations.

### 14.7.4 Program Brief Analysis

* **launch File Analysis**

The launch files are located at: **~/ros2_ws/src/example/example/stepper/tag_sorting_stepper.launch.py**

**1. launch_setup Function**

<img class="common_img" src="../_static/media/chapter_14/media/image65.png" style="width:700px" />

It loads the file **launch/depth_camera.launch.py** from the **peripherals** package and starts the depth camera node, providing RGB images and depth data as visual input for tag recognition, such as QR codes, barcodes, or specific markers. It loads the file **launch/armpi_ultra.launch.py** from the **sdk** package and starts the robotic arm’s low-level control services, including servo driving and kinematics computation, providing hardware support for grasping objects with tags. It launches the executable **tag_sorting_stepper** from the **example** package. It uses the tag information recognized by the camera, such as tag type and ID, to control the stepper motor and drive the mechanical structure, transporting objects with different tags to the corresponding sorting areas and completing automated sorting, with logs output to the screen.

**2. generate_launch_description Function**

<img class="common_img" src="../_static/media/chapter_14/media/image27.png" style="width:500px" />

Creates and returns a `LaunchDescription` object, calling `launch_setup` via OpaqueFunction as the standard entry point for the ROS 2 launch file.

**3. Main Function**

<img class="common_img" src="../_static/media/chapter_14/media/image28.png" style="width:500px" />

Creates a LaunchDescription object and a LaunchService service. The launch description is added to the service and executed, enabling the startup of the entire system manually.

* **Python File Analysis**

Python files locate at: **~/ros2_ws/src/example/example/stepper/include/tag_sorting_stepper.py**

**1. Import the Necessary Libraries**

<img class="common_img" src="../_static/media/chapter_14/media/image66.png" style="width:500px" />

1.  **`sdk.common`**: Custom SDK modules for common utilities.

2.  **`app.common.Heart`**: A heartbeat module in the application used to monitor system status.

3.  **`dt_apriltags.Detector`**: AprilTag detector used for recognizing and tracking AprilTags.

4.  **`servo_controller_msgs.msg.ServosPosition`**: A custom ROS message type representing servo positions.

5.  **`kinematics.kinematics_control.set_pose_target`**: A function from the kinematics control module used to set pose targets.

6.  **`servo_controller.bus_servo_control.set_servo_position`**: Function for setting servo positions in the servo control module.

**2. Target Position Specification**

<img class="common_img" src="../_static/media/chapter_14/media/image67.png" style="width:500px" />

This is a dictionary used to store target positions for different tags. This structure simplifies quickly locating the corresponding position based on tag during the robot’s tasks. Sliding rail position range is from 0 (leftmost end) to 5200 (rightmost end).

**3. TagSorting Class**

<img class="common_img" src="../_static/media/chapter_14/media/image68.png" style="width:700px" />

<img class="common_img" src="../_static/media/chapter_14/media/image69.png" style="width:700px" />

It initializes the tag sorting node. It sets AprilTag-related parameters, such as tag size and detector configuration, initializes state variables and counters, configures the stepper motor controller, and creates the image queue and thread lock. It also sets up ROS2 publishers, subscribers, and service clients, and starts the initialization process via a timer. The AprilTag detector is configured with parameters such as the tag36h11 family to improve detection accuracy.

**4. init_process Function**

<img class="common_img" src="../_static/media/chapter_14/media/image70.png" style="width:700px" />

It cancels the initialization timer, resets the stepper motor to zero steps, and adjusts the robotic arm servos to the initial pose, waiting 1.5 seconds to ensure they reach the position. It launches the main loop thread main and the transport control thread transport_thread, preparing to start tag detection and sorting.

**5. send_request Function**

<img class="common_img" src="../_static/media/chapter_14/media/image32.png" style="width:500px" />

It encapsulates the service call logic, sends the request, and waits synchronously for the result to ensure reliable communication with services such as robotic arm kinematics and stepper motor control.

**6. camera_info_callback Function**

<img class="common_img" src="../_static/media/chapter_14/media/image53.png" style="width:500px" />

It receives the camera intrinsic message, extracts the camera matrix intrinsic and distortion coefficients distortion, and provides the basic parameters for converting pixel coordinates to world coordinates.

**7. image_callback Function**

<img class="common_img" src="../_static/media/chapter_14/media/image71.png" style="width:500px" />

It converts ROS image messages sensor_msgs/Image to OpenCV format (BGR) and stores them in the image queue, discarding old images if the queue is full, for the main loop to process tag detection.

**8. get_roi Function**

<img class="common_img" src="../_static/media/chapter_14/media/image72.png" style="width:700px" />

It loads the configuration file **transform.yaml** and parses the calibration parameters, including corner coordinates, extrinsics, and the poses of the white area. It projects the reference points in the world coordinate system onto the image plane using the camera intrinsics and extrinsics, calculates the ROI regions to limit the detection range, reduce background interference, and improve tag recognition efficiency.

**9. get_object_world_position Function**

<img class="common_img" src="../_static/media/chapter_14/media/image73.png" style="width:700px" />

It calculates the world coordinates of the tag using the camera intrinsics intrinsic and extrinsics extristric through a projection matrix. It applies the scaling factors and offsets from the calibration file **calibration.yaml** to optimize coordinate accuracy and returns the three-dimensional position of the tag in the world coordinate system, providing precise coordinates for the robotic arm to grasp.

**10. calculate_pick_grasp_yaw Function**

<img class="common_img" src="../_static/media/chapter_14/media/image57.png" style="width:700px" />

It calculates the grasp angle yaw based on the object’s world coordinates, considering the gripper size and target pose, and optimizes the grasp direction to ensure stable tag grasping.

**11. calculate_place_grasp_yaw Function**

<img class="common_img" src="../_static/media/chapter_14/media/image58.png" style="width:500px" />

It calculates the placement angle to ensure the tag is accurately placed in the target sorting area.

**12. transport_thread Function**

<img class="common_img" src="../_static/media/chapter_14/media/image74.png" style="width:700px" />

<img class="common_img" src="../_static/media/chapter_14/media/image75.png" style="width:700px" />

When start_transport is True, it executes the full pick-transport-place process, and calls pick_and_place.pick to control the robotic arm to grasp the tag based on the calculated world coordinates and angle. It moves the tag to the corresponding position using the stepper motor service move_stepper, retrieving the steps from TARGET_POSITION according to the tag ID. It calls pick_and_place.place to control the robotic arm to place the tag. It resets the stepper motor, returns the robotic arm to the initial pose, and resets the state variables, setting start_transport to False.

**13. main Function**

<img class="common_img" src="../_static/media/chapter_14/media/image76.png" style="width:700px" />

<img class="common_img" src="../_static/media/chapter_14/media/image77.png" style="width:500px" />

It retrieves images from the image queue and uses the dt_apriltags detector to recognize AprilTags, extracting the tag ID, center coordinates, angle, and other information. It triggers the transport process by setting start_transport to True when a tag remains stable for 20 consecutive frames with displacement less than 0.001 meters. It resets the target if the position continues to move for more than 10 frames. It displays the annotated images in real time, showing tag contours, centers, and ROI regions for debugging and monitoring.

**14. Main Function**

<img class="common_img" src="../_static/media/chapter_14/media/image78.png" style="width:500px" />

It serves as the entry point for starting and running the tag sorting ROS2 node. It creates an instance of TagSorting, uses a MultiThreadedExecutor to support multithreaded processing, starts the node, and ensures proper resource cleanup upon termination.
